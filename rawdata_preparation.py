# -*- coding: utf-8 -*-
"""
Created on Mon Mar 18 10:08:35 2019

@author: An-Sheng

This script aims to rename the (common) sprctra file names by their composite depth, which is gnerated by this script. 
Instead of renaming directly, the script copy and name the new file.
Also, This script collect all result.txt files generated by Itrax core scanner and build a single dataset.
Mwanwhile, there are many naming inconsistencies, so I also output some errors to record them. 
"""

################## Import Packages #################### 
import time
import os
import shutil
import glob
import pandas as pd
import numpy as np

####################################################### 

###### Set the working directory ######
path = '~\\GeopolarLabor\\#Projekte\\WASA\\XRF\\data_composite'
os.chdir(path)

start = time.time()
date = time.strftime('%Y%m%d', time.localtime())
         
###### Input WASA_Umrechnung_Kerntiefen_20190313.xlsx ######
# sheet: Section length
lith_raw_df = (
        pd.read_excel(
                'WASA_Umrechnung_Kerntiefen_20190313.xlsx', 
                sheet_name = 'Section length', 
                skipfooter = 4 # Number of lines at bottom of file to skip
                )
        .drop(['Core status', 'Top section Liner length', 'Notes'], axis = 'columns') 
        ) # "Drop" the needless columns 

# replace the blank in the columns name by _
cols = {'Kernbezeichnung': 'core_ID'}
for col in lith_raw_df.columns[1:]:
    cols.update(
            {col: col.lower().replace(' ', '_')}
            )
# remove the zero length core
lith_df = (
        lith_raw_df[lith_raw_df['length core'] != 0]
        .reset_index(drop = True) # drop the length core column which is 0
        .rename(columns = cols) # rename it by the cols dict
        )
# replace the nan in "section 0" by 0. this can help later process
lith_df.length_section_0 = lith_df.length_section_0.fillna(0)

# uppercase all the core ID 
for i in range(len(lith_df.core_ID)):
    lith_df.loc[i, 'core_ID'] = lith_df.core_ID[i].upper() # Use 'loc' for fixing the index (could be easier just used the core_ID[i])

# compare the excel's core list and the list in the data_original folder (data_composite and data_original)
# https://www.geeksforgeeks.org/python-set-difference/
e = set(lith_df.core_ID)
o = set(os.listdir('..\\data_original'))

print('There are some cores that in the excel but not in the original_data:\n', e-o, end = '\n\n')
print('There is a core that in the original_data but not in the excel:', o-e)

del e, o

# The core N24 is excluded becuase its sections 2~4 are missing.
lith_df = lith_df[lith_df['core_ID'] != 'N24'].reset_index(drop = True)

#####  Generate the composite start of each section ######
# create an empty dataframe
lith_composite_df = pd.DataFrame(index = lith_df.core_ID, 
                                 columns = ['top_of_s{}'.format(i) for i in range(6)]) 


for row in range(len(lith_df)):
    core_length_mm = int( (lith_df.length_core[row] - lith_df.air_at_top_section[row]) * 10 )
    lith_composite_df.iloc[row, 0] = core_length_mm - int( lith_df.iloc[row, 1] * 10 )
    for col in range(1, int( lith_df.loc[row, 'top_section']) + 1):
        lith_composite_df.iloc[row, col] = lith_composite_df.iloc[row, col - 1] - int( lith_df.iloc[row, col + 1] * 10 )
    if lith_composite_df.iloc[row, 0] == core_length_mm:
        lith_composite_df.iloc[row, 0] = 0

lith_composite_df.to_csv('composite_depth_of_the_cores_{}.csv'.format(date))
    


###### Input all result.txt iteratively ######
result_dir_list = glob.glob('..\\data_original\\*\\**\\*.xrf\\result.txt', recursive = True )

# check if there is any duplicate dirs
if len(pd.Series(result_dir_list).unique()) == len(result_dir_list):
    print('no duplicate dirs')
else: print('some duplicate dirs...need to figure out')

lazy_scan = [] 

# list these problematic cores (two sections scanned in the same time, fragemental scanned)
for section in glob.glob('..\\data_original\\*\\*\\*,*bit.tif'):
    lazy_scan.append(section.split('\\')[3])

# W3-2 section has 4 subsection, so I remove it now and later append those subsections.    
lazy_scan.remove('W3-2')

# core N24 is excluded due to its fragmental scanning, and VVC07 is due to the lack of core description. 
for i in ['N24-1', 'N24-4', 'N24-5', 'VVC07-1', 'VVC07-2', 'VVC07-3', 'VVC07-4', 'W3-2A', 'W3-2B', 'W3-2C', 'W3-2D']:
    lazy_scan.append(i)


result_mega_df = pd.DataFrame()
problems = []
server_dir = []
composite_depth_mm = []
new_spe_dir = []
core_IDS = []
core_sectionS = []
result_count = []

for txt in result_dir_list:
    
    # catch the core_ID & core_section 
    core_section = txt.split('\\')[-2][:-4].upper()
    
    # deal with the inconsistency of the naming....
    if 'VVC' in core_section:
        core_ID = core_section[:5]
    elif len(core_section) == 4:
        core_ID = core_section[:2]
    elif len(core_section) == 6:
        core_ID = core_section[:4]
    else:
        core_ID = core_section[:3]
    
    if core_section not in lazy_scan:     # only calculate the sections without problem 
        try:
            result_df = pd.read_table(txt, skiprows = 2).drop('Unnamed: 54', axis = 'columns')
        except KeyError:
            problems.append([core_section, 'the setting is different--> inconsistent columns'])
        else:
            for row in range(len(result_df)):
                f = result_df.loc[row, 'filename'].split('\\')
                ser_dir = path[: -14] + txt[3: -10] + 'XRF data\\' + f[-1]
                server_dir.append(ser_dir)
                
                # deal with the inconsistency of the naming....
                if '(' in core_section:
                    col = lith_composite_df.columns[ int(core_section[-4]) ]
                elif ('-' in core_section) & ('_' in core_section):
                    col = lith_composite_df.columns[ int(core_section[-3]) ]
                else:
                    col = lith_composite_df.columns[ int(core_section[-1]) ]
                
                comp_depth = int(
                        result_df.loc[row, 'position (mm)'] + lith_composite_df.loc[core_ID, col]
                        )
                composite_depth_mm.append(comp_depth)
                
                new_dir = (
                        path[: -14] + 'data_composite\\' + core_ID + '\\XRF data'
                        )
                new_spe = (
                        new_dir + '\\{}_{:0>6}.spe'.format(core_ID, comp_depth)
                        )
                new_spe_dir.append(new_spe)
                
                core_IDS.append(core_ID)
                core_sectionS.append(core_section)
                
                # copy the spe to desired data_composite folder
                try:
                    if os.path.isfile(new_spe) == False:    # some spe has been copied at previous runs (1. druing debug 2. overlape scan)
                        if os.path.exists(new_dir):   # if the directory isn't create, create one
                            shutil.copy(ser_dir, new_spe)
                        else:
                            os.makedirs(new_dir, 0o775)
                            shutil.copy(ser_dir, new_spe)
                except FileNotFoundError:
                    problems.append([core_section, 'the original_data has some problems, so it is not copied'])
    
                del ser_dir, comp_depth, new_spe
            
            result_count.append(core_section)
            result_mega_df = result_mega_df.append(result_df, ignore_index = True)
            print(core_section, end = ',\t')
                
result_mega_df['core_ID'] = core_IDS
result_mega_df['core_section'] = core_sectionS
result_mega_df['composite_depth_mm'] = composite_depth_mm
result_mega_df['data_original_dir'] = server_dir
result_mega_df['new_spe_dir'] = new_spe_dir


date = time.strftime('%Y%m%d', time.localtime())
result_mega_df.to_csv(path + '\\WASA_all_xrf_result_{}.csv'.format(date), index = False)
problems
end = time.time()

dur = (end-start)/60
print('{} result.txt are copied and renamed in composite depth'.format(len(result_count)))
print('Process took: {:.2f} mins.'.format(dur))

if os.path.isfile('problems_{}.txt'.format(date)):
    os.remove('problems_{}.txt'.format(date))
with open('problems_{}.txt'.format(date), 'a') as f:
    for line in problems:
        print(line, file = f)
        
if os.path.isfile('processed_sections_{}.txt'.format(date)):
    os.remove('processed_sections_{}.txt'.format(date))
with open('processed_sections_{}.txt'.format(date), 'a') as f:
    for section in result_count:
        print(section, file = f)

# check if there is negative composite depth
with open('negative_depth_spe_{}.txt'.format(date), 'a') as f:
    for spe in result_mega_df.new_spe_dir[result_mega_df.composite_depth_mm < 0]:
        print(spe.split('\\')[-1], file = f)

